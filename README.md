# nanoGPT
This project is a smaller version of GPT that I am building to learn how large language models work. I am implementing everything from scratch step by step — from tokenization, dataset processing, transformers, attention, training, and inference — all the way to a working mini GPT model.

I am learning from Andrej Karpathy and following his lectures and walkthroughs while experimenting and building my own version alongside. The goal is to understand every component deeply rather than just using prebuilt frameworks.

This is a learning-oriented project and will evolve continuously until a full mini-GPT implementation is complete.
